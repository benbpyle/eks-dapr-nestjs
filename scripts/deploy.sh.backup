#!/bin/bash

# Kubernetes + Dapr + Datadog Deployment Script
# This script automates the entire deployment process

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Configuration
CLUSTER_NAME="dapr-demo-final"
REGION="us-west-2"
NAMESPACE="dapr-services"
RUST_IMAGE="public.ecr.aws/f8u4w2p3/dapr-comms"
NODE_IMAGE="public.ecr.aws/f8u4w2p3/dapr-api"

# Function to print colored output
print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check if command exists
check_command() {
    if ! command -v $1 &> /dev/null; then
        print_error "$1 is not installed. Please install it first."
        exit 1
    fi
}

# Function to wait for deployment to be ready
wait_for_deployment() {
    local namespace=$1
    local deployment=$2
    local timeout=${3:-300}

    print_status "Waiting for deployment $deployment in namespace $namespace to be ready..."
    kubectl wait --for=condition=available --timeout=${timeout}s deployment/$deployment -n $namespace
}

# Function to wait for dapr to be ready
wait_for_dapr() {
    print_status "Waiting for Dapr to be ready..."
    kubectl wait --for=condition=available --timeout=300s deployment/dapr-operator -n dapr-system
    kubectl wait --for=condition=available --timeout=300s deployment/dapr-sidecar-injector -n dapr-system
    kubectl wait --for=condition=available --timeout=300s deployment/dapr-sentry -n dapr-system
}

# Main deployment function
main() {
    print_status "Starting Kubernetes + Dapr + Datadog deployment..."

    # Check prerequisites
    print_status "Checking prerequisites..."
    check_command "kubectl"
    check_command "eksctl"
    check_command "helm"
    check_command "docker"
    check_command "dapr"
    check_command "aws"

    # Check for docker buildx
    if ! docker buildx version &> /dev/null; then
        print_error "Docker buildx is not available. Please ensure Docker Desktop is running or buildx is installed."
        exit 1
    fi

    # Check AWS credentials
    if ! aws sts get-caller-identity &> /dev/null; then
        print_error "AWS credentials not configured. Please run 'aws configure' first."
        exit 1
    fi

    print_success "All prerequisites met!"

    # Step 1: Create EKS Cluster
    print_status "Step 1: Creating EKS cluster..."
    if eksctl get cluster --name $CLUSTER_NAME --region $REGION &> /dev/null; then
        print_warning "Cluster $CLUSTER_NAME already exists, checking nodegroups..."

        # Check if nodegroups exist
        if ! eksctl get nodegroup --cluster $CLUSTER_NAME --region $REGION &> /dev/null; then
            print_status "No nodegroups found. Creating managed nodegroup..."
            if eksctl create nodegroup --cluster $CLUSTER_NAME --region $REGION --name mng-x86-dapr --node-type m5.large --nodes 2 --nodes-min 1 --nodes-max 4; then
                print_success "Nodegroup created successfully!"
            else
                print_error "Failed to create nodegroup"
                exit 1
            fi
        else
            print_success "Nodegroups already exist"
        fi
    else
        print_status "Creating EKS cluster (this may take 15-20 minutes)..."
        print_status "This includes both control plane and managed nodegroup creation..."

        # Create cluster with proper error handling
        if eksctl create cluster -f kubernetes/cluster-config.yaml; then
            print_success "EKS cluster created successfully!"
        else
            print_error "EKS cluster creation timed out or failed"
            print_status "Checking if cluster was partially created..."

            if eksctl get cluster --name $CLUSTER_NAME --region $REGION &> /dev/null; then
                print_warning "Cluster exists but may be incomplete. Checking nodegroups..."

                # Check if nodegroups exist
                if ! eksctl get nodegroup --cluster $CLUSTER_NAME --region $REGION &> /dev/null; then
                    print_status "No nodegroups found. Creating managed nodegroup..."
                    eksctl create nodegroup --cluster $CLUSTER_NAME --region $REGION --name mng-x86-dapr --node-type m5.large --nodes 2 --nodes-min 1 --nodes-max 4
                fi
            else
                print_error "Cluster creation completely failed"
                exit 1
            fi
        fi
    fi

    # Update kubeconfig
    print_status "Updating kubeconfig..."
    eksctl utils write-kubeconfig --cluster=$CLUSTER_NAME --region=$REGION

    # Verify cluster connection and wait for nodes
    print_status "Verifying cluster connection and waiting for nodes..."
    for i in {1..30}; do
        if kubectl get nodes &> /dev/null; then
            node_count=$(kubectl get nodes --no-headers | wc -l)
            if [ "$node_count" -ge 1 ]; then
                print_success "Cluster connection verified with $node_count nodes"
                break
            fi
        fi

        if [ "$i" -eq 30 ]; then
            print_error "Cannot connect to cluster or no nodes found after 5 minutes"
            exit 1
        fi

        print_status "Waiting for cluster connectivity... (attempt $i/30)"
        sleep 10
    done

    # Step 1b: Install Core EKS Addons (Critical Fix)
    print_status "Step 1b: Installing core EKS addons..."

    # Install vpc-cni addon
    print_status "Installing VPC-CNI addon..."
    eksctl create addon --cluster $CLUSTER_NAME --region $REGION --name vpc-cni --version latest || print_warning "VPC-CNI addon may already exist"

    # Install CoreDNS addon
    print_status "Installing CoreDNS addon..."
    eksctl create addon --cluster $CLUSTER_NAME --region $REGION --name coredns --version latest || print_warning "CoreDNS addon may already exist"

    # Install kube-proxy addon
    print_status "Installing kube-proxy addon..."
    eksctl create addon --cluster $CLUSTER_NAME --region $REGION --name kube-proxy --version latest || print_warning "kube-proxy addon may already exist"

    print_success "Core EKS addons installed!"

    # Wait for nodes to become Ready
    print_status "Waiting for worker nodes to become Ready..."
    for i in {1..30}; do
        ready_nodes=$(kubectl get nodes --no-headers | grep -c " Ready ")
        if [ "$ready_nodes" -ge 2 ]; then
            print_success "Worker nodes are Ready!"
            break
        elif [ "$i" -eq 30 ]; then
            print_error "Timeout waiting for nodes to become Ready"
            exit 1
        fi
        print_status "Waiting for nodes... ($ready_nodes/2 ready)"
        sleep 10
    done

    # Step 1c: Associate OIDC Provider (CRITICAL for EBS CSI Driver and service accounts)
    print_status "Step 1c: Associating OIDC provider for IAM service accounts..."
    OIDC_ISSUER=$(aws eks describe-cluster --name $CLUSTER_NAME --region $REGION --query 'cluster.identity.oidc.issuer' --output text)
    OIDC_ID=$(echo $OIDC_ISSUER | cut -d'/' -f5)

    if ! aws iam list-open-id-connect-providers --query "OpenIDConnectProviderList[?contains(Arn, '$OIDC_ID')].Arn" --output text | grep -q "arn:aws:iam"; then
        print_status "Associating IAM OIDC provider..."
        if ! eksctl utils associate-iam-oidc-provider --region=$REGION --cluster=$CLUSTER_NAME --approve; then
            print_error "Failed to associate OIDC provider"
            exit 1
        fi
        print_success "OIDC provider associated successfully"
    else
        print_success "OIDC provider already associated"
    fi

    # Step 2: Install AWS Load Balancer Controller
    print_status "Step 2: Installing AWS Load Balancer Controller..."

    # Create IAM service account
    print_status "Checking for AWS Load Balancer Controller service account..."
    if ! eksctl get iamserviceaccount --cluster $CLUSTER_NAME --name aws-load-balancer-controller -n kube-system &> /dev/null; then
        print_status "Creating IAM service account for AWS Load Balancer Controller..."
        eksctl create iamserviceaccount \
            --cluster=$CLUSTER_NAME \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --role-name AmazonEKSLoadBalancerControllerRole-${CLUSTER_NAME} \
            --attach-policy-arn=arn:aws:iam::aws:policy/ElasticLoadBalancingFullAccess \
            --override-existing-serviceaccounts \
            --approve
        print_success "IAM service account created"
    else
        print_success "IAM service account already exists"
    fi

    # Install AWS Load Balancer Controller
    print_status "Adding EKS Helm repository..."
    helm repo add eks https://aws.github.io/eks-charts
    helm repo update

    print_status "Checking for AWS Load Balancer Controller installation..."
    if ! helm list -n kube-system | grep aws-load-balancer-controller &> /dev/null; then
        print_status "Installing AWS Load Balancer Controller..."
        helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n kube-system \
            --set clusterName=$CLUSTER_NAME \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller
        print_success "AWS Load Balancer Controller installed"
    else
        print_success "AWS Load Balancer Controller already installed"
    fi

    print_success "AWS Load Balancer Controller installed!"

    # Step 3: Install Datadog Operator
    print_status "Step 3: Installing Datadog Operator..."

    helm repo add datadog https://helm.datadoghq.com
    helm repo update

    kubectl create namespace datadog-operator --dry-run=client -o yaml | kubectl apply -f -

    print_status "Checking for Datadog Operator installation..."
    if ! helm list -n datadog-operator | grep datadog-operator &> /dev/null; then
        print_status "Installing Datadog Operator..."
        helm install datadog-operator datadog/datadog-operator -n datadog-operator
        wait_for_deployment "datadog-operator" "datadog-operator"
        print_success "Datadog Operator installed"
    else
        print_success "Datadog Operator already installed"
    fi

    # Apply Datadog secret and agent
    print_status "Applying Datadog configuration..."
    kubectl apply -f kubernetes/datadog/datadog-secret.yaml
    kubectl apply -f kubernetes/datadog/datadog-agent.yaml

    print_success "Datadog Operator installed!"

    # Step 4: Install EBS CSI Driver (CRITICAL for Dapr scheduler)
    print_status "Step 4: Installing EBS CSI Driver..."

    print_status "Checking if EBS CSI driver addon exists..."
    if ! eksctl get addon --cluster $CLUSTER_NAME --region $REGION | grep aws-ebs-csi-driver &> /dev/null; then
        print_status "Installing EBS CSI driver addon..."
        eksctl create addon --cluster $CLUSTER_NAME --name aws-ebs-csi-driver --region $REGION
        print_success "EBS CSI driver installed"

        # Wait for EBS CSI driver to be ready
        print_status "Waiting for EBS CSI driver to be ready..."
        kubectl wait --for=condition=ready pods -l app=ebs-csi-controller -n kube-system --timeout=300s
        print_success "EBS CSI driver ready"
    else
        print_success "EBS CSI driver already installed"
    fi

    # Set default StorageClass to prevent PVC binding issues
    print_status "Setting gp2 as default StorageClass..."
    kubectl patch storageclass gp2 -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'
    print_success "Default StorageClass configured"

    # Step 5: Install Dapr
    print_status "Step 5: Installing Dapr..."

    if ! kubectl get namespace dapr-system &> /dev/null; then
        dapr init -k
        wait_for_dapr

        # Wait for Dapr scheduler pods specifically (these were the problem!)
        print_status "Waiting for Dapr scheduler pods to be ready..."
        for i in {1..60}; do
            scheduler_ready=$(kubectl get pods -n dapr-system -l app=dapr-scheduler-server --no-headers 2>/dev/null | grep -c "1/1.*Running" || echo "0")
            if [ "$scheduler_ready" -ge 3 ]; then
                print_success "All 3 Dapr scheduler pods are ready!"
                break
            elif [ "$i" -eq 60 ]; then
                print_warning "Dapr scheduler pods took longer than expected, but continuing deployment..."
                print_status "Current scheduler pod status:"
                kubectl get pods -n dapr-system -l app=dapr-scheduler-server || true
                break
            fi
            print_status "Waiting for Dapr scheduler pods... ($scheduler_ready/3 ready)"
            sleep 10
        done
    else
        print_warning "Dapr already installed, skipping..."
    fi

    print_success "Dapr installed!"

    # Step 6: Build and Push Docker Images
    print_status "Step 6: Building and pushing Docker images..."

    # Login to ECR Public
    print_status "Logging into ECR Public..."
    if aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws; then
        print_success "Successfully logged into ECR Public"
    else
        print_error "Failed to login to ECR Public"
        exit 1
    fi

    # Create ECR repositories if they don't exist
    print_status "Creating ECR repositories if needed..."
    aws ecr-public create-repository --region us-east-1 --repository-name dapr-comms &>/dev/null || print_status "dapr-comms repository already exists"
    aws ecr-public create-repository --region us-east-1 --repository-name dapr-api &>/dev/null || print_status "dapr-api repository already exists"

    # Build and push Rust service (with x86_64 architecture)
    print_status "Building Rust service with explicit x86_64 architecture..."
    cd services/comms

    # Use simplified Dockerfile and multiarch build for platform compatibility
    if docker buildx build --platform linux/amd64 -t ${RUST_IMAGE}:latest --push -f Dockerfile.simple .; then
        print_success "Rust service built and pushed successfully"
    else
        print_error "Failed to build/push Rust service"
        exit 1
    fi
    cd ../..

    # Build and push Node service (with x86_64 architecture)
    print_status "Building Node.js service with explicit x86_64 architecture..."
    cd services/greeter

    # Use multiarch build for platform compatibility
    if docker buildx build --platform linux/amd64 -t ${NODE_IMAGE}:latest --push .; then
        print_success "Node.js service built and pushed successfully"
    else
        print_error "Failed to build/push Node.js service"
        exit 1
    fi
    cd ../..

    print_success "All Docker images built and pushed successfully!"
    print_status "Images available at:"
    echo "  - ${RUST_IMAGE}:latest"
    echo "  - ${NODE_IMAGE}:latest"

    # Step 7: Deploy Services
    print_status "Step 7: Deploying services to Kubernetes..."

    # Create namespace
    kubectl apply -f kubernetes/namespaces/dapr-services-namespace.yaml

    # Apply Dapr components and configuration to the correct namespace
    print_status "Applying Dapr components and configuration..."
    kubectl apply -f dapr-components/config.yaml -n $NAMESPACE
    kubectl apply -f dapr-components/redis-statestore.yaml -n $NAMESPACE

    # Deploy services
    kubectl apply -f kubernetes/services/greeter-service.yaml
    kubectl apply -f kubernetes/services/comms-service.yaml

    # Wait for deployments to be ready with proper timeout and error checking
    print_status "Waiting for service deployments to be ready..."
    for deployment in greeter comms; do
        print_status "Waiting for $deployment deployment..."
        if ! kubectl wait --for=condition=available --timeout=300s deployment/$deployment -n $NAMESPACE; then
            print_warning "$deployment deployment not ready within timeout, checking pod status..."
            kubectl get pods -n $NAMESPACE -l app=$deployment
            kubectl describe pods -n $NAMESPACE -l app=$deployment | tail -20
        else
            print_success "$deployment deployment is ready!"
        fi
    done

    # Apply ingress
    kubectl apply -f kubernetes/ingress/ingress.yaml

    print_success "Services deployed successfully!"

    # Step 8: Get service information
    print_status "Step 8: Getting service information..."

    echo ""
    print_success "Deployment completed successfully! ðŸŽ‰"
    echo ""
    print_status "Service Information:"
    echo "â”œâ”€â”€ Cluster: $CLUSTER_NAME"
    echo "â”œâ”€â”€ Region: $REGION"
    echo "â”œâ”€â”€ Namespace: $NAMESPACE"
    echo "â””â”€â”€ Services: comms, greeter"
    echo ""

    print_status "Getting ALB URL (may take a few minutes to provision)..."
    echo "Run this command to get the ALB URL when ready:"
    echo "kubectl get ingress -n $NAMESPACE"
    echo ""

    print_status "Testing locally first:"
    echo "kubectl port-forward -n $NAMESPACE svc/comms-service 8080:80"
    echo "curl 'http://localhost:8080/greet?name=World'"
    echo ""

    print_status "Monitor in Datadog:"
    echo "â”œâ”€â”€ Site: https://us5.datadoghq.com"
    echo "â”œâ”€â”€ APM: https://us5.datadoghq.com/apm/services"
    echo "â””â”€â”€ Infrastructure: https://us5.datadoghq.com/infrastructure"
    echo ""

    print_success "Happy monitoring! ðŸ“Š"
}

# Cleanup function
cleanup() {
    print_warning "Cleaning up resources..."
    print_status "To delete the cluster run: eksctl delete cluster --name $CLUSTER_NAME --region $REGION"
}

# Trap cleanup on exit
trap cleanup EXIT

# Run main function
main "$@"
